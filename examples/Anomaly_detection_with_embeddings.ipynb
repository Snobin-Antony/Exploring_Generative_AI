{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkzOKBirz271"
      },
      "source": [
        "# Anomaly detection with embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWzQEqNosrS"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Anomaly_detection_with_embeddings.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQPvHyHCz7mk"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the embeddings from the Gemini API to detect potential outliers in your dataset. You will visualize a subset of the 20 Newsgroup dataset using [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) and detect outliers outside a particular radius of the central point of each categorical cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LyLLYVEhzud8"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0kitgd5aLG"
      },
      "source": [
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6OeEZ5Bj5Zr8"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "client = genai.Client(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhWtEhZ6BO58"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "The [20 Newsgroups Text Dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) from the open-source [SciKit project](https://scikit-learn.org/) contains 18,000 newsgroups posts on 20 topics divided into training and test sets. The split between the training and test datasets are based on messages posted before and after a specific date. This tutorial uses the training subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtHABp9BBTIt"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
        "\n",
        "# View list of class names for dataset\n",
        "newsgroups_train.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPKgmQDQC3zd"
      },
      "source": [
        "Here is the first example in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSXYP0JwBXHh"
      },
      "outputs": [],
      "source": [
        "idx = newsgroups_train.data[0].index(\"Lines\")\n",
        "print(newsgroups_train.data[0][idx:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Raafa2naC6Ec"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Apply functions to remove names, emails, and extraneous words from data points in newsgroups.data\n",
        "newsgroups_train.data = [\n",
        "    re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", d) for d in newsgroups_train.data\n",
        "]  # Remove email\n",
        "newsgroups_train.data = [\n",
        "    re.sub(r\"\\([^()]*\\)\", \"\", d) for d in newsgroups_train.data\n",
        "]  # Remove names\n",
        "newsgroups_train.data = [\n",
        "    d.replace(\"From: \", \"\") for d in newsgroups_train.data\n",
        "]  # Remove \"From: \"\n",
        "newsgroups_train.data = [\n",
        "    d.replace(\"\\nSubject: \", \"\") for d in newsgroups_train.data\n",
        "]  # Remove \"\\nSubject: \"\n",
        "\n",
        "# Cut off each text entry after 5,000 characters\n",
        "newsgroups_train.data = [\n",
        "    d[0:5000] if len(d) > 5000 else d for d in newsgroups_train.data\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjE_Lsr6IhEd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Put training points into a dataframe\n",
        "df_train = pd.DataFrame(newsgroups_train.data, columns=[\"Text\"])\n",
        "df_train[\"Label\"] = newsgroups_train.target\n",
        "# Match label to target name index\n",
        "df_train[\"Class Name\"] = df_train[\"Label\"].map(\n",
        "    newsgroups_train.target_names.__getitem__\n",
        ")\n",
        "\n",
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7OHvTBaImpB"
      },
      "source": [
        "Next, sample some of the data by taking 150 data points in the training dataset and choosing a few categories. This tutorial uses the science categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPxwl05BIjWX"
      },
      "outputs": [],
      "source": [
        "# Take a sample of each label category from df_train\n",
        "SAMPLE_SIZE = 150\n",
        "df_train = (\n",
        "    df_train.groupby(\"Label\", as_index=False)\n",
        "    .apply(lambda x: x.sample(SAMPLE_SIZE))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Choose categories about science\n",
        "df_train = df_train[df_train[\"Class Name\"].str.contains(\"sci\")]\n",
        "\n",
        "# Reset the index\n",
        "df_train = df_train.reset_index()\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjTrEnmdIo5P"
      },
      "outputs": [],
      "source": [
        "df_train[\"Class Name\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUgv8SOwXfAX"
      },
      "source": [
        "## Create the embeddings\n",
        "\n",
        "In this section, you will see how to generate embeddings for the different texts in the dataframe using the embeddings from the Gemini API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01I9uzbo4t26"
      },
      "source": [
        "### API changes to Embeddings with model embedding-001\n",
        "\n",
        "For the embeddings model, `text-embedding-004`, there is a task type parameter and the optional title (only valid with task_type=`RETRIEVAL_DOCUMENT`).\n",
        "\n",
        "These parameters apply only to the embeddings models. The task types are:\n",
        "\n",
        "Task Type | Description\n",
        "---       | ---\n",
        "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
        "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.\n",
        "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
        "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
        "CLUSTERING\t| Specifies that the embeddings will be used for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkS_EWfAXcxc"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from google.genai import types\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from google.api_core import retry\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def make_embed_text_fn(model):\n",
        "\n",
        "    @retry.Retry(timeout=300.0)\n",
        "    def embed_fn(texts: list[str]) -> list[list[float]]:\n",
        "        # Set the task_type to CLUSTERING and embed the batch of texts\n",
        "        embeddings = client.models.embed_content(\n",
        "            model=model,\n",
        "            contents=texts,\n",
        "            config=types.EmbedContentConfig(task_type=\"CLUSTERING\"),\n",
        "        ).embeddings\n",
        "        return np.array([embedding.values for embedding in embeddings])\n",
        "\n",
        "    return embed_fn\n",
        "\n",
        "\n",
        "def create_embeddings(df):\n",
        "    MODEL_ID = \"text-embedding-004\" # @param [\"embedding-001\",\"text-embedding-004\"] {allow-input: true}\n",
        "    model = f\"models/{MODEL_ID}\"\n",
        "    embed_fn = make_embed_text_fn(model)\n",
        "\n",
        "    batch_size = 100  # at most 100 requests can be in one batch\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Loop over the texts in chunks of batch_size\n",
        "    for i in tqdm(range(0, len(df), batch_size)):\n",
        "        batch = df[\"Text\"].iloc[i : i + batch_size].tolist()\n",
        "        embeddings = embed_fn(batch)\n",
        "        all_embeddings.extend(embeddings)\n",
        "\n",
        "    df[\"Embeddings\"] = all_embeddings\n",
        "    return df\n",
        "\n",
        "\n",
        "df_train = create_embeddings(df_train)\n",
        "df_train.drop(\"index\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNTjKcD_aluG"
      },
      "source": [
        "## Dimensionality reduction\n",
        "\n",
        "The dimension of the document embedding vector is 768. In order to visualize how the embedded documents are grouped together, you will need to apply dimensionality reduction as you can only visualize the embeddings in 2D or 3D space. Contextually similar documents should be closer together in space as opposed to documents that are not as similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDHDQmeZqy2"
      },
      "outputs": [],
      "source": [
        "len(df_train[\"Embeddings\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5-XU-twaoK6"
      },
      "outputs": [],
      "source": [
        "# Convert df_train['Embeddings'] Pandas series to a np.array of float32\n",
        "X = np.array(df_train[\"Embeddings\"].to_list(), dtype=np.float32)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV-Y7iEtbAkm"
      },
      "source": [
        "You will apply the t-Distributed Stochastic Neighbor Embedding (t-SNE) approach to perform dimensionality reduction. This technique reduces the number of dimensions, while preserving clusters (points that are close together stay close together). For the original data, the model tries to construct a distribution over which other data points are \"neighbors\" (e.g., they share a similar meaning). It then optimizes an objective function to keep a similar distribution in the visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhYKF-lObC04"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(random_state=0, max_iter=1000)\n",
        "tsne_results = tsne.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31wdqnp_bH9B"
      },
      "outputs": [],
      "source": [
        "df_tsne = pd.DataFrame(tsne_results, columns=[\"TSNE1\", \"TSNE2\"])\n",
        "df_tsne[\"Class Name\"] = df_train[\n",
        "    \"Class Name\"\n",
        "]  # Add labels column from df_train to df_tsne\n",
        "df_tsne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTj8HfhpbJ9X"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))  # Set figsize\n",
        "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
        "sns.scatterplot(data=df_tsne, x=\"TSNE1\", y=\"TSNE2\", hue=\"Class Name\", palette=\"Set2\")\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.title(\"Scatter plot of news using t-SNE\")\n",
        "plt.xlabel(\"TSNE1\")\n",
        "plt.ylabel(\"TSNE2\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQbX4pcMdBe"
      },
      "source": [
        "## Outlier detection\n",
        "\n",
        "To determine which points are anomalous, you will determine which points are inliers and outliers. Start by finding the centroid, or location that represents the center of the cluster, and use the distance to determine the points that are outliers.\n",
        "\n",
        "Start by getting the centroid of each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUIkLxtMK4qC"
      },
      "outputs": [],
      "source": [
        "def get_centroids(df_tsne):\n",
        "    # Get the centroid of each cluster\n",
        "    centroids = df_tsne.groupby(\"Class Name\").mean()\n",
        "    return centroids\n",
        "\n",
        "\n",
        "centroids = get_centroids(df_tsne)\n",
        "centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJH4Oo6E-r_6"
      },
      "outputs": [],
      "source": [
        "def get_embedding_centroids(df):\n",
        "    emb_centroids = dict()\n",
        "    grouped = df.groupby(\"Class Name\")\n",
        "    for c in grouped.groups:\n",
        "        sub_df = grouped.get_group(c)\n",
        "        # Get the centroid value of dimension 768\n",
        "        emb_centroids[c] = np.mean(sub_df[\"Embeddings\"], axis=0)\n",
        "\n",
        "    return emb_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tas9Yg4_iyq"
      },
      "outputs": [],
      "source": [
        "emb_c = get_embedding_centroids(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMvdYLjKl32a"
      },
      "source": [
        "Plot each centroid you have found against the rest of the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpN02WY3Ogji"
      },
      "outputs": [],
      "source": [
        "# Plot the centroids against the cluster\n",
        "fig, ax = plt.subplots(figsize=(8, 6))  # Set figsize\n",
        "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
        "sns.scatterplot(data=df_tsne, x=\"TSNE1\", y=\"TSNE2\", hue=\"Class Name\", palette=\"Set2\")\n",
        "sns.scatterplot(\n",
        "    data=centroids,\n",
        "    x=\"TSNE1\",\n",
        "    y=\"TSNE2\",\n",
        "    color=\"black\",\n",
        "    marker=\"X\",\n",
        "    s=100,\n",
        "    label=\"Centroids\",\n",
        ")\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.title(\"Scatter plot of news using t-SNE with centroids\")\n",
        "plt.xlabel(\"TSNE1\")\n",
        "plt.ylabel(\"TSNE2\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onFfUf1XoEQW"
      },
      "source": [
        "Choose a radius. Anything beyond this bound from the centroid of that category is considered an outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87cDfNpvOu7f"
      },
      "outputs": [],
      "source": [
        "def calculate_euclidean_distance(p1, p2):\n",
        "    return np.sqrt(np.sum(np.square(p1 - p2)))\n",
        "\n",
        "\n",
        "def detect_outlier(df, emb_centroids, radius):\n",
        "    for idx, row in df.iterrows():\n",
        "        class_name = row[\"Class Name\"]  # Get class name of row\n",
        "        # Compare centroid distances\n",
        "        dist = calculate_euclidean_distance(\n",
        "            row[\"Embeddings\"], emb_centroids[class_name]\n",
        "        )\n",
        "        df.at[idx, \"Outlier\"] = dist > radius\n",
        "\n",
        "    return len(df[df[\"Outlier\"] == True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsVsod5MKd3X"
      },
      "outputs": [],
      "source": [
        "range_ = np.arange(0.3, 0.75, 0.02).round(decimals=2).tolist()\n",
        "num_outliers = []\n",
        "for i in range_:\n",
        "    num_outliers.append(detect_outlier(df_train, emb_c, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vReUSOjbNHQv"
      },
      "outputs": [],
      "source": [
        "# Plot range_ and num_outliers\n",
        "fig = plt.figure(figsize=(14, 8))\n",
        "plt.rcParams.update({\"font.size\": 12})\n",
        "plt.bar(list(map(str, range_)), num_outliers)\n",
        "plt.title(\"Number of outliers vs. distance of points from centroid\")\n",
        "plt.xlabel(\"Distance\")\n",
        "plt.ylabel(\"Number of outliers\")\n",
        "for i in range(len(range_)):\n",
        "    plt.text(i, num_outliers[i], num_outliers[i], ha=\"center\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNISxrzwGvBH"
      },
      "source": [
        "Depending on how sensitive you want your anomaly detector to be, you can choose which radius you would like to use. For now, 0.62 is used, but you can change this value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMNFFSDOTELn"
      },
      "outputs": [],
      "source": [
        "# View the points that are outliers\n",
        "RADIUS = 0.62\n",
        "detect_outlier(df_train, emb_c, RADIUS)\n",
        "df_outliers = df_train[df_train[\"Outlier\"] == True]\n",
        "df_outliers.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_wbM5yYE4MS"
      },
      "outputs": [],
      "source": [
        "# Use the index to map the outlier points back to the projected TSNE points\n",
        "outliers_projected = df_tsne.loc[df_outliers[\"Outlier\"].index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCt4wfYdoTJz"
      },
      "source": [
        "Plot the outliers and denote them using a transparent red color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrAKwBp0TaNu"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))  # Set figsize\n",
        "plt.rcParams.update({\"font.size\": 10})\n",
        "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
        "sns.scatterplot(data=df_tsne, x=\"TSNE1\", y=\"TSNE2\", hue=\"Class Name\", palette=\"Set2\")\n",
        "sns.scatterplot(\n",
        "    data=centroids,\n",
        "    x=\"TSNE1\",\n",
        "    y=\"TSNE2\",\n",
        "    color=\"black\",\n",
        "    marker=\"X\",\n",
        "    s=100,\n",
        "    label=\"Centroids\",\n",
        ")\n",
        "# Draw a red circle around the outliers\n",
        "sns.scatterplot(\n",
        "    data=outliers_projected,\n",
        "    x=\"TSNE1\",\n",
        "    y=\"TSNE2\",\n",
        "    color=\"red\",\n",
        "    marker=\"o\",\n",
        "    alpha=0.5,\n",
        "    s=90,\n",
        "    label=\"Outliers\",\n",
        ")\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.title(\"Scatter plot of news with outliers projected with t-SNE\")\n",
        "plt.xlabel(\"TSNE1\")\n",
        "plt.ylabel(\"TSNE2\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVm_A9HmGwEN"
      },
      "source": [
        "Use the index values of the datafames to print a few examples of what outliers can look like in each category. Here, the first data point from each category is printed out. Explore other points in each category to see data that are deemed as outliers, or anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpZ-hcDvG13M"
      },
      "outputs": [],
      "source": [
        "sci_crypt_outliers = df_outliers[df_outliers[\"Class Name\"] == \"sci.crypt\"]\n",
        "print(sci_crypt_outliers[\"Text\"].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPsQB3eHJN25"
      },
      "outputs": [],
      "source": [
        "sci_elec_outliers = df_outliers[df_outliers[\"Class Name\"] == \"sci.electronics\"]\n",
        "print(sci_elec_outliers[\"Text\"].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APPg8TURJ9yt"
      },
      "outputs": [],
      "source": [
        "sci_med_outliers = df_outliers[df_outliers[\"Class Name\"] == \"sci.med\"]\n",
        "print(sci_med_outliers[\"Text\"].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeoJF7c8KB49"
      },
      "outputs": [],
      "source": [
        "sci_space_outliers = df_outliers[df_outliers[\"Class Name\"] == \"sci.space\"]\n",
        "print(sci_space_outliers[\"Text\"].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siaPlEJhh0pr"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You've now created an anomaly detector using embeddings! Try using your own textual data to visualize them as embeddings, and choose some bound such that you can detect outliers. You can perform dimensionality reduction in order to complete the visualization step. Note that t-SNE is good at clustering inputs, but can take a longer time to converge or might get stuck at local minima. If you run into this issue, another technique you could consider are [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Anomaly_detection_with_embeddings.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "google": {
      "image_path": "/examples/anomaly_detection_files/output_IrAKwBp0TaNu_0.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}